CQELS Cloud Performance Tests

= Introduction =

Linked Stream Data extends the Linked Data paradigm to dynamic data sources. It enables the integration and joint processing of heterogeneous stream data sources and data from the Linked Data Cloud. Several Linked Stream Data processing engines exist but their scalability still needs to be in improved in terms of (static and dynamic) data sizes, number of concurrent queries, stream update frequencies, etc. Moreover, none of them yet supports parallel processing in the Cloud, i.e., elastic load profiles in a hosted environment. To remedy these limitations, we present an approach for elastically parallelizing the continuous ex- ecution of queries over Linked Stream Data. For this, we have developed novel and highly efficient and scalable parallel algorithms for continuous query operators. Our approach and algorithms are implemented in our CQELS Cloud system and we present extensive evaluations of its supe- rior performance on Amazon EC2 demonstrating its high scalability and excellent elasticity.

In this tutorial we will describe the required steps for setting up a distributed, multinode CQELS Cloud cluster backed by the Storm, HBase and Hadoop Distributed File System, running on local cluster and Amazon EC2.

=Tutorial approach and structure =

We implemented our elastic execution model and the parallel algorithms using ZooKeeper, Storm and HBase. The architecture of CQELS Cloud is is shown in Figure 1: The Execution Coordinator coordinates the cluster of OCs using coordination services provided by Storm and HBase which share the same Zookeeper cluster. The Global Scheduler uses Nimbus3, an open source EC2/S3- compatible Infrastructure-as-a-Service implementation, to deploy the operators’ code to OCs and monitor for failures. Each OC node runs a Storm supervisor which listens for continuous processing tasks assigned to its machine via Nimbus. The processing tasks that need to process the persistent data use the HBase Client component to access data stored in HBase. The machines running an OC also hosts the HDFS DataNodes of the HBase cluster. The DataNodes are accessed via the OC’s HRegionServer component of HBase.

[http://cqels.googlecode.com/files/CQELSCloud.png]

=How-to: Set Up an Apache Hadoop/Apache HBase Cluster on EC2=

==Prerequisites==

 * Amazon EC2 account
 * Whirr-0.8.0-cdh4.2.0
 * Storm 0.8.2

===Step 1: Get the Cluster Running===
I’m going to assume you already have an Amazon Web Services account (because it’s awesome, and the basic tier is free.) If you don’t, go get one. Amazon’s directions for getting started are pretty clear, or you can easily find a guide with Google. We won’t actually be interacting with the Amazon management console much, but you will need two pieces of information, your AWS Access Key ID and your AWS Secret Access Key.

To find these, go to [https://portal.aws.amazon.com/gp/aws/securityCredentials Amazon Credentials]. You can write these down, or better yet add them to your shell startup script by doing:


Add your content here.  Format your content with:
  * Text in *bold* or _italic_
  * Headings, paragraphs, and lists
  * Automatic links to other wiki pages